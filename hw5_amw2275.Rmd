---
title: "Homework 5"
author: "Adam Whalen"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(rvest)
library(patchwork)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_color_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1

Read in the data.

```{r}
homicide_df = 
  read_csv("./data/homicide_data/homicide-data.csv") %>% 
  mutate(
    city_state = str_c(city, state, sep = "_"),
    resolved = case_when(
      disposition == "Closed without arrest" ~ "unsolved",
      disposition == "Open/No arrest"        ~ "unsolved",
      disposition == "Closed by arrest"      ~ "solved",
    )
  ) %>% 
  select(city_state, resolved) %>% 
  filter(city_state != "Tulsa_AL")
```

Let's look at this a bit.

```{r, message = FALSE}
aggregate_df = 
  homicide_df %>% 
  group_by(city_state) %>% 
  summarize(
    hom_total = n(),
    hom_unsolved = sum(resolved == "unsolved")
  )
```

Can I do a prop test for a single city?

```{r}
prop.test(
  aggregate_df %>% filter(city_state == "Baltimore_MD") %>% pull(hom_unsolved),
  aggregate_df %>% filter(city_state == "Baltimore_MD") %>% pull(hom_total)
  ) %>% 
  broom::tidy()
```

Try to iterate.

```{r}
results_df = 
  aggregate_df %>% 
  mutate(
    prop_tests = map2(.x = hom_unsolved, .y = hom_total, ~prop.test(x = .x, n = .y)),
    tidy_tests = map(.x = prop_tests, ~broom::tidy(.x))
  ) %>% 
  select(-prop_tests) %>% 
  unnest(tidy_tests) %>% 
  select(city_state, estimate, conf.low, conf.high)
```


```{r}
results_df %>% 
  mutate(city_state = fct_reorder(city_state, estimate)) %>% 
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

```{r, eval = FALSE}
homicide_df = 
  read_csv("./data/homicide-data.csv") %>% 
  mutate(
    city_state = str_c(city, state, sep = "_"),
    resolved = case_when(
      disposition == "Closed without arrest" ~ "unsolved",
      disposition == "Open/No arrest"        ~ "unsolved",
      disposition == "Closed by arrest"      ~ "solved",
    )
  ) %>% 
  select(city_state, resolved) %>% 
  filter(city_state != "Tulsa_AL") %>% 
  nest(data = resolved)
```

## Problem 2: Longitudinal Data Set

Import one dataset first.

```{r, path_1}
data_1 = read_csv("data/long_data/con_01.csv")
```

Cool, now let's make a data frame of the paths.

```{r, path_df}
path_df = 
  tibble(
    path = list.files("data/long_data")
  ) %>% 
  mutate(path = str_c("data/long_data/", path))

read_csv(path_df$path[[1]])
```

Ok, let's try this as a `for` loop.

```{r path_for, message = FALSE}
output_df = vector("list", length = 20)

for (i in 1:20) {
  
  output_df[[i]] = read_csv(path_df$path[[i]])
  
}

output_df %>% bind_rows()
```

Good, but maybe a map will be easier.

```{r path_map, message = FALSE}
output_df = map_df(path_df$path, read_csv) %>% bind_rows()
```

It was! Ok, now to put it all into the `path_df` data frame and tidy.

```{r path_tidy, message = FALSE}
path_tidy = 
  path_df %>% 
  mutate(
    data = map(.x = path, ~read_csv(.x)),
    arm = 
      case_when(
        str_detect(path, "con") == TRUE ~ "control",
        str_detect(path, "exp") == TRUE ~ "experimental"
      ),
    id = str_sub(path, -6, -5),
    id = as.factor(id)
  ) %>% 
  select(id, arm, data) %>% 
  unnest(data) %>% 
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    names_prefix = "week_",
    values_to = "value"
  ) %>% 
  mutate(
    week = as.numeric(week),
    value = as.numeric(value)
  )

path_tidy
```

Sweet! Now let's make a spaghetti plot.

```{r spagett}
spagett_plot = 
  path_tidy %>% 
  group_by(arm, id) %>% 
  ggplot(aes(x = week, y = value, group = id, color = id)) +
  geom_line() +
  geom_point(alpha = 0.5) +
  facet_grid(arm ~ .) +
  labs(
    title = "Observed Values for Control vs. Experimental Arms over Time",
    x = "Values",
    y = "Week of Observation"
  )

spagett_plot
```

Based on our spaghetti plot, it appears that generally, the experimental group had an increase in values over time, whereas the control group had a slight decrease in values over the course of observation. There was substantial variation within each group for any given week, from spreads of roughly 2.5 to 5 units.

## Problem 3: Simulation

To begin, we will set a random sample that has 5000 datasets taken from a normal distribution with a mean of 0, sample size of 30, and standard deviation of 5. 


```{r, set_fxn}
power_sim = function(mu, samp_size = 30, sigma = 5) {
  
  if (!is.numeric(mu)) {
    stop("Mean input must be numeric")
  }
  
  sim_data = 
    tibble(
      x = rnorm(mean = mu, n = samp_size, sd = sigma)
    )
  
  result = 
    sim_data %>% 
    summarize(
      mu_hat = mean(x),
      sd_hat = sd(x)
    ) %>% 
    t.test() %>% 
    broom::tidy() %>% 
    janitor::clean_names() %>% 
    mutate(
      mu_hat = estimate
    ) %>% 
    select(mu_hat, p_value)
  
  return(result)
}

power_sim(0)
```

Rad, I built a function. Now, to loop it for µ = 0. Note: I will be leaving this preliminary loop at 50 runs to preserve computing power.

```{r loop_1}
output = vector("list", length = 50)

for (i in 1:50) {
  
  output[[i]] = power_sim(mu = 0)
  
}

bind_rows(output)
```

Cool. Repeat for µ = [1, 2, 3, 4, 5, 6] as well. Note: I will be leaving this preliminary loop at 50 runs to preserve computing power.

```{r list_loop}
mu_list = 
  tibble(
    "mu = 0" = 0,
    "mu = 1" = 1,
    "mu = 2" = 2,
    "mu = 3" = 3,
    "mu = 4" = 4,
    "mu = 5" = 5,
    "mu = 6" = 6
  )

output = vector("list", length = 7)

for (i in 1:7) {
  
  output[[i]] = 
    rerun(50, power_sim(mu = mu_list[[i]])) %>% 
    bind_rows()
  
}

output
```

Great. Now let's put all of it into one data frame, with 5000 runs for each µ.

```{r df_mapping}
sim_df = 
  tibble(
    mu = c(0, 1, 2, 3, 4, 5, 6)
  ) %>% 
  mutate(
    output_lists = map(.x = mu, ~ rerun(5000, power_sim(mu = .x))),
    power_df = map(output_lists, bind_rows)
  ) %>% 
  select(-output_lists) %>% 
  unnest(power_df)

sim_df
```

Lit. Now let's manipulate the data to calculate power, and prepare it for graphing.

```{r power_plot, warning = FALSE}
power_plot = 
  sim_df %>% 
  group_by(mu) %>% 
  count(reject_null = p_value < 0.05) %>% 
  mutate(
    power = n/sum(n)
  ) %>% 
  filter(reject_null == TRUE) %>% 
  ggplot(aes(x = mu, y = power)) +
  geom_point() +
  geom_smooth() +
  labs(
    title = "Simulated Power on True Effect Size",
    x = "True µ of Simulated Sample",
    y = "Estimated Power"
  ) + 
  xlim(0, 6)

power_plot
```

Our plot of the estimated power for our test (proportion of the time we reject the null hypothesis when it is false) shows a sigmoid curve, that starts close to 0 for a true µ of 1, but increases exponentially as the effect size increases. We would expect this, as our ability to correctly reject the null (µ = 0) increases as the true µ moves away from 0. 

```{r patch_plot, message = FALSE, warning = FALSE}
mu_hat_plot = 
  sim_df %>% 
  group_by(mu) %>% 
  summarize(avg_mu_hat = mean(mu_hat)) %>% 
  ggplot(aes(x = mu, y = avg_mu_hat)) +
  geom_point() +
  geom_smooth() +
  labs(
    x = "True µ of Simulated Sample",
    y = "Average Estimate for µ"
  ) 

muhat_reject_plot = 
  sim_df %>% 
  filter(p_value < 0.05) %>% 
  group_by(mu) %>% 
  summarize(avg_mu_hat = mean(mu_hat)) %>% 
  ggplot(aes(x = mu, y = avg_mu_hat)) +
  geom_point() +
  geom_smooth() +
  labs(
    x = "True µ of Simulated Sample",
    y = "Average Estimate for µ when p < 0.05"
  ) 

mu_hat_plot + muhat_reject_plot +
  plot_annotation(
    title = "Estimated µ on True µ",
    subtitle = "Comparing overall simulation to those when the null hypothesis was rejected"
  )
```

From our overlay plots of estimated µ compared to the true µ, when all simulations are included versus when only those which resulted in a rejection of the null hypothesis, we can see that the full plot appears to be linear, whereas the plot with only rejections included shows an average estimated µ that is higher for lower true µ values. We would also expect this, as the average rejected µ estimate needs to be significantly different from the null value of 0 to be rejected, and the further away it is, the more likely it is to be rejected at the 5% level of significance. Therefore, the estimated µs will be higher at lower true µ samples.


